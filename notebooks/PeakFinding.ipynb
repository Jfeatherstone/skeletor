{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092110b-e7c7-4a54-b38a-5459a2a9fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import skeletor as sk\n",
    "from skeletor.data import loadTestDataset\n",
    "from skeletor.spatial import angularHistogramAroundPoint\n",
    "\n",
    "#from skeletor.spatial import approxPeakFind\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "from pepe.topology import findPeaksMulti, findPeaks1D, findPeaks2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409303f-2649-417d-8faf-ca96f0f9b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialClusterLabels(points, l=.001):\n",
    "    \"\"\"\n",
    "    Partition a set of points in clusters, and return the\n",
    "    cluster label for each point.\n",
    "\n",
    "    Method is very simplistic: the neighbors (points within\n",
    "    a threshold distance) of a point are added to the same\n",
    "    cluster, and the process is repeated until no points\n",
    "    remain. This means that the size of clusters can be\n",
    "    larger than this distance threshold (eg. think of a\n",
    "    chain of points).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : numpy.ndarray[N,d]\n",
    "        Array of N points in d-dimensions\n",
    "\n",
    "    l : float\n",
    "        Distance threshold to consider two points as being in the\n",
    "        same cluster. Given as fraction of the total system size.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    labels : numpy.ndarray[N]\n",
    "        Array of N labels (integers) denoting cluster\n",
    "        assignment. Total number of clusters will be `max(labels)+1`.\n",
    "    \"\"\"\n",
    "    d = np.shape(points)[-1]\n",
    "    \n",
    "    systemLengthScale = np.sqrt(np.sum([(np.max(points[:,i]) - np.min(points[:,i]))**2 for i in range(d)]))\n",
    "    threshold = l*systemLengthScale\n",
    "    \n",
    "    # Generate a kd-tree\n",
    "    kdTree = KDTree(points)\n",
    "\n",
    "    pointsList = points.tolist()\n",
    "    randomOrder = np.arange(len(pointsList))\n",
    "    np.random.shuffle(randomOrder)\n",
    "    \n",
    "    labels = np.zeros(len(pointsList)) - 1\n",
    "    labelsToMerge = []\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while len(np.where(labels == -1)[0]) > 0:\n",
    "        # Grab a point, if it doesn't have a group already, create a new one\n",
    "        if labels[randomOrder[i]] == -1:\n",
    "            labels[randomOrder[i]] = np.max(labels)+1\n",
    "        p = pointsList[randomOrder[i]]\n",
    "\n",
    "        # Find its neighbors\n",
    "        neighbors = kdTree.query_ball_point(p, r=threshold)\n",
    "\n",
    "        # neighorLabels = [labels[n] for n in neighors]\n",
    "        # uniqueNeighborLabels = np.unique(neighborLabels)\n",
    "        # if len(uniqueNeighborLabels) == 1:\n",
    "        #     # If all are unlabeled\n",
    "        #     if uniqueNeighborLabels[0] == -1:\n",
    "        #         # Create a new group\n",
    "        #         labels[randomOrder[i]] = np.max(labels)+1\n",
    "\n",
    "        #     # All are already in another group\n",
    "        #     else:\n",
    "        #         # Assign this point to that group as well\n",
    "        #         labels[randomOrder[i]] = uniqueNeighborLabels[0]\n",
    "        \n",
    "        # Assign all of them to be the same group\n",
    "        for index in neighbors:\n",
    "            if index == randomOrder[i]:\n",
    "                continue\n",
    "                \n",
    "            # If they already have a label, we will eventually want to merge the labels\n",
    "            if labels[index] >= 0 and labels[index] != labels[randomOrder[i]]:\n",
    "                # See if this is already in a merge group\n",
    "                inGroup = False\n",
    "                for j in range(len(labelsToMerge)):\n",
    "                    # If it is already going to be merged, add the\n",
    "                    # new label to this group\n",
    "                    if labels[index] in labelsToMerge[j]:\n",
    "                        inGroup = True\n",
    "                        # If the new label is not in the merge group, add it\n",
    "                        if not labels[randomOrder[i]] in labelsToMerge[j]:\n",
    "                            labelsToMerge[j].append(labels[randomOrder[i]])\n",
    "                            \n",
    "                        break\n",
    "                # If we aren't in a merge group, we should create\n",
    "                # a new one\n",
    "                if not inGroup:\n",
    "                    labelsToMerge.append([labels[index], labels[randomOrder[i]]])\n",
    "                    \n",
    "            elif labels[index] == -1:\n",
    "                labels[index] = labels[randomOrder[i]]\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "    mergedLabels = np.zeros_like(labels) - 1\n",
    "    \n",
    "    # Now we have to sort out merged clusters\n",
    "    labelSets = []\n",
    "    for i in range(int(np.max(labels))+1):\n",
    "        # Generate a list of all of the labels that are identified together\n",
    "        # including this value of i\n",
    "        allLabels = [i]\n",
    "        for j in range(len(labelsToMerge)):\n",
    "            if i in labelsToMerge[j]:\n",
    "                allLabels += labelsToMerge[j]\n",
    "\n",
    "        # sort so that way we can do a unique check to remove\n",
    "        # duplicates\n",
    "        allLabels = sorted(np.unique(allLabels))\n",
    "        # Python may automatically collapse the list if it is of length 1,\n",
    "        # which we don't want.\n",
    "        labelSets.append([allLabels] if len(allLabels) == 1 else allLabels)\n",
    "\n",
    "    # Remove duplicate sets, creating disjoint sets of labels\n",
    "    uniqueSets = np.unique(np.array(labelSets, list))\n",
    "\n",
    "    # If there are no overlaps, then this above operation will\n",
    "    # flatten the array to be one dimensional. We can't do anything\n",
    "    # about that, except to check if it is the case. The good news is\n",
    "    # that if this is the case, then we already have disjoint sets\n",
    "    # and we can just immediately return our label array\n",
    "    if len(np.shape(uniqueSets)) == 1:\n",
    "        return labels\n",
    "\n",
    "    # Relabel things with the new disjoint sets\n",
    "    mergedLabels = np.zeros_like(labels)\n",
    "\n",
    "    # TODO: Probably a much better way to do this\n",
    "    for i in range(len(pointsList)):\n",
    "        for j in range(len(uniqueSets)):\n",
    "            if labels[i] in uniqueSets[j]:\n",
    "                mergedLabels[i] = j\n",
    "                break\n",
    "\n",
    "    return mergedLabels\n",
    "\n",
    "def spatialClusterCenters(points, l=.001, return_weights=False):\n",
    "    \"\"\"\n",
    "    Partition a set of points in clusters, and compute the\n",
    "    center of each cluster.\n",
    "\n",
    "    Generates clusters using `pepe.topology.spatialClusterLabels()`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : numpy.ndarray[N,d]\n",
    "        Array of N points in d-dimensions\n",
    "\n",
    "    l : float\n",
    "        Distance threshold to consider two points as being in the\n",
    "        same cluster. Given as fraction of the total system size.\n",
    "\n",
    "    return_weights : bool\n",
    "        Whether to return the weight -- defined as the fraction of\n",
    "        points included in that cluster -- alongside the centers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    centers : numpy.ndarray[N, d]\n",
    "        Array of N points in d-dimensions representing the detected\n",
    "        clusters in the system.\n",
    "\n",
    "    weights : numpy.ndarray[N]\n",
    "        Array of weights -- defined as fraction of all points included\n",
    "        in each cluster -- for each cluster. Only returned if\n",
    "        `return_weights=True`.\n",
    "\n",
    "    \"\"\"\n",
    "    labels = spatialClusterLabels(points, l=l)\n",
    "    numLabels = int(np.max(labels))+1\n",
    "    \n",
    "    # Compute the center of each cluster\n",
    "    weights = np.zeros(numLabels)\n",
    "    centers = np.zeros((numLabels, np.shape(points)[-1]))\n",
    "    \n",
    "    for i in range(numLabels):\n",
    "        indices = np.where(labels == i)\n",
    "        centers[i] = np.mean(np.array(points)[indices], axis=0)\n",
    "        weights[i] = len(indices[0])/len(labels)\n",
    "\n",
    "    order = np.argsort(weights)[::-1]\n",
    "    centers = centers[order]\n",
    "    weights = weights[order]\n",
    "    \n",
    "    if return_weights:\n",
    "        return centers, weights\n",
    "        \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08518e1-4c2a-498c-a313-2bbee841201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to test on\n",
    "points = loadTestDataset('2d_curve_2', extraNoise=.02)\n",
    "testPoint = 25\n",
    "neighborDistance = 0.1\n",
    "\n",
    "hist, axes = angularHistogramAroundPoint(points, testPoint, neighborDistance=neighborDistance)\n",
    "\n",
    "plt.plot(hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd730c-176c-444d-8614-47cadcd28042",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = find_peaks(hist, prominence=0.5)[0]\n",
    "approxPeaks, prevalences = findPeaks1D(hist, minPeakPrevalence=.1, periodic=False)\n",
    "\n",
    "plt.plot(hist)\n",
    "for i in peaks:\n",
    "    plt.axvline(i, linestyle='--', c='red', alpha=.5)\n",
    "\n",
    "for i in approxPeaks:\n",
    "    plt.axvline(i, linestyle='--', c='blue', alpha=.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51b361-a2a1-49ff-becc-f16bdfe22a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 2, 3]) % np.array([2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe16d50-e186-46ed-91c4-8e23eeb41272",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit findPeaks1D(hist, minPeakPrevalence=0.6)\n",
    "%timeit findPeaksMulti(hist, minPeakPrevalence=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6780b-c7fc-4ffd-b077-239cb622d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to test on\n",
    "points = loadTestDataset('wireframe_cube_1', extraNoise=.005)\n",
    "testPoint = 100\n",
    "neighborDistance = 0.1\n",
    "\n",
    "hist, axes = angularHistogramAroundPoint(points, testPoint, neighborDistance=neighborDistance)\n",
    "\n",
    "plt.imshow(hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a447ba2-1b16-4c6f-8fdb-2f2d20ea1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array(np.where(hist > 4*np.mean(hist))).T\n",
    "print(points.shape)\n",
    "centers = spatialClusterCenters(points, l=1)\n",
    "plt.scatter(*points.T)\n",
    "plt.scatter(*centers.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988b5cd-6fb8-4696-82f2-857151686247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8af973-d36e-49e8-b887-8c47f3d0ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "??spatialClusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bd78a-7ce5-4346-9abd-04906604de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks, prevalences = findPeaks2D(hist, minPeakPrevalence=.6, periodic=False)\n",
    "\n",
    "plt.imshow(hist)\n",
    "plt.scatter(*peaks.T[::-1], c='tab:red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a083c8-644e-44d6-81e2-3e8ecbc86600",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit findPeaks2D(hist, minPeakPrevalence=0.6)\n",
    "%timeit findPeaksMulti(hist, minPeakPrevalence=0.6)\n",
    "%timeit approxPeakFind(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce24b13-b074-40c0-894b-73034e605c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "findPeaksMulti(hist, minPeakPrevalence=0.6, allowOptimize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b0dfb-b317-4735-b5a8-28bc9011b8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
